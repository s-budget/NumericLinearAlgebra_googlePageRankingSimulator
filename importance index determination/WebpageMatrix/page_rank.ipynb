{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Page Rank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we will implement the Google Page Rank algorithm. The algorithm is used to rank web pages based on their importance, considering both the number and quality of incoming links.\n",
    "\n",
    "## Modeling Internet pages\n",
    "\n",
    "Since the whole Internet is consisted of the web pages which are linking other web sites and pages, we can represent the whole Internet as a graph defined with matrix.\n",
    "\n",
    "This matrix, often referred to as the adjacency matrix, captures the relationships between different web pages based on their hyperlinks. Each row and column in the matrix represents a web page, and the entries indicate whether a hyperlink exists between two pages. By representing web pages as a matrix, we can analyze the intricate network of interconnections, enabling us to understand the relationships and navigate the vast web of information effectively. This matrix-based representation serves as a fundamental building block for various algorithms, including the Google Page Rank algorithm, that aim to measure and rank the importance of web pages based on their connectivity patterns.\n",
    "\n",
    "## Solving the Page Rank\n",
    "\n",
    "To quantify this for a web of n pages, let $L_k$ ⊂ {1, 2, . . . , n} denote the set of pages with a link to page k, that is, $L_k$ is the set of page k’s backlinks. For each k we require\n",
    "\n",
    "$$ \n",
    "x_k = \\sum_{j \\in L_k} \\frac{x_j}{n_j}\n",
    "$$\n",
    "\n",
    "where $n_j$ is the number of outgoing links from page j (which must be positive since if j ∈ $L_k$ , then page j links to at least page k). We will assume that a link from a page to itself will not be counted.\n",
    "\n",
    "Solving this linear problem can be done using matrix calculation as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{x} = \\mathbf{x}\n",
    "$$\n",
    "\n",
    "One way to look at this problem is as finding the eigenvectors of the matrix $\\mathbf{A}$ for the eigenvalue 1 which can be done in mupltiple different ways, but today we are going to use iterative \"power\" method.\n",
    "\n",
    "Unfortunatlly, as nice as this seems, there are few problems that can occur in the real world. \n",
    "Those problems are:\n",
    "+ **Subwebs** - Not all parts of the internet are connected to eachother and there will be many dissconnected subwebs that create the whole Internet. However, this results only in dimensionality of our eigenvector; dimension of the eigenvector will be the same as the number of subwebs; each of the dimensions will be the resulting score for each subweb.\n",
    "+ **Dangling nodes** - Some web pages will have no links to other pages which makes them a dangling node. This causes some problems when solving the Page Rank.\n",
    "\n",
    "There is a nice way of avoiding this problems while still getting good Page Ranks for the pages. Let's define matrix $\\mathbf{S}$ which is n x n with all entries $1/n$. Now we can replace our matrix $\\mathbf{A}$ with:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = (1 - m)\\mathbf{A} + m\\mathbf{S}\n",
    "$$\n",
    "\n",
    "where \\(0 \\leq m \\leq 1\\). This creates matrix with node dangling nodes and now can be used to calculate \"importance score\" of pages.\n",
    "\n",
    "> **Note:** This was a simple explanation of how the Page Rang works, but there is a lot more mathematical theory as in why does this work and some definitions of the column-stochastic matrices and other important details that can be read in the \"The $25,000,000,000 Eigenvector: The Linear Algebra behind Google\" article by Kurt Bryan and Tanya Leise.\n",
    "\n",
    "## Loading Required Libraries\n",
    "\n",
    "To begin, we need to import the necessary libraries. In this case, we will be using the `numpy` library for efficient numerical computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Scraped Matrix\n",
    "\n",
    "Next, we load the scraped matrix, which represents the connectivity between different web pages. This matrix will serve as the basis for calculating the page ranks.\n",
    "\n",
    "This matrix was made by scraping \"https://solo-leveling.fandom.com\" with the maximum depth of 3 so that the matrix isn't too big and has as least as possible dangling nodes (pages that don't link to any other page) and subwebs (smaller parts of web that have no links between each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259, 1259)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Matrix.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    web_sites = np.array(lines[0].strip().split(\",\"))\n",
    "    weights = []\n",
    "    for line in lines[1:]:\n",
    "        weights.append(list(map(lambda x: float(x), line.strip().split(\",\"))))\n",
    "\n",
    "    M = np.array(weights)\n",
    "M.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions for Calculating Page Scores\n",
    "\n",
    "We define a set of functions to calculate the scores of the web pages. These functions will help us determine the importance and ranking of each page based on the connectivity information provided by the matrix.\n",
    "These functions can also be used for determening eigenvalues and eigenvectors for any given matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "n = M.shape[0]\n",
    "x_0 = np.ones((n, 1))\n",
    "\n",
    "def get_max_eigenvalue(M, niter=1000):\n",
    "    # Procedure for finding the largest eigenvalue and corresponding eigenvector\n",
    "    for _ in range(niter):\n",
    "        x_k1 = M.dot(x_0)\n",
    "        x_0 = x_k1 / np.linalg.norm(x_k1)\n",
    "\n",
    "    alpha = (x_0.T.dot(M).dot(x_0)) / (x_0.T.dot(x_0))\n",
    "    return x_0, alpha\n",
    "\n",
    "def get_min_eigenvalue(M, niter=1000):\n",
    "    # Inverse procedure for finding the smallest eigenvalue and corresponding eigenvector\n",
    "    n = M.shape[0]\n",
    "    B = np.linalg.inv(M)\n",
    "    x_0 = np.ones((n, 1))\n",
    "    for _ in range(niter):\n",
    "        x_k1 = B.dot(x_0)\n",
    "        x_0 = x_k1 / np.linalg.norm(x_k1)\n",
    "\n",
    "    beta = (x_0.T.dot(B).dot(x_0)) / (x_0.T.dot(x_0))\n",
    "    smallest_eigenvalue = 1 / beta\n",
    "    return x_0, smallest_eigenvalue\n",
    "\n",
    "def get_eigenvalue_omega(M, omega, niter=1000):\n",
    "    # Procedure for obtaining the eigenvector for a chosen eigenvalue omega\n",
    "    n = M.shape[0]\n",
    "    x_0 = np.ones((n, 1))\n",
    "    for _ in range(niter):\n",
    "        alpha_prev = (x_0.T.dot(M).dot(x_0)) / (x_0.T.dot(x_0))\n",
    "        x_k1 = np.linalg.inv(M - omega * np.eye(n)).dot(x_0)\n",
    "        x_0 = x_k1 / np.linalg.norm(x_k1)\n",
    "        alpha = (x_0.T.dot(M).dot(x_0)) / (x_0.T.dot(x_0))\n",
    "\n",
    "        if np.abs(alpha - alpha_prev) < 1e-6:\n",
    "            break\n",
    "    eigenvalue_omega = (x_0.T.dot(M).dot(x_0)) / (x_0.T.dot(x_0))\n",
    "    eigenvector_omega = x_0\n",
    "    return eigenvector_omega, eigenvalue_omega\n",
    "\n",
    "def diagonalize(M, niter=1000):\n",
    "    # Procedure for diagonalizing the matrix using QR decomposition\n",
    "    A1 = M.copy()\n",
    "    for _ in range(niter):\n",
    "        Q, R = np.linalg.qr(A1)\n",
    "        A1 = R.dot(Q)\n",
    "    return A1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Page Ranks and displaying Top Pages\n",
    "\n",
    "Using the defined functions, we calculate the page ranks for each web page in the matrix. The page ranks are a measure of the relative importance of each page within the network of pages.\n",
    "\n",
    "Finally, we display the top pages based on their page ranks. These pages are considered to be the most important and influential within the network.\n",
    "\n",
    "By implementing the Google Page Rank algorithm, we can gain insights into the importance and ranking of web pages based on their connectivity patterns. Let's proceed with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98737689]]\n",
      "Eigenvalue: [[0.94263209]]\n",
      "First 10 pages base on Google Page rank system and their scores:\n",
      " [['https://solo-leveling.fandom.com/createnewwiki.fandom.com/Special:CreateNewWiki'\n",
      "  '0.08233887857265766']\n",
      " ['https://solo-leveling.fandom.com/wiki/Local_Sitemap'\n",
      "  '0.08210631288539828']\n",
      " ['https://solo-leveling.fandom.com/f' '0.08199355355402307']\n",
      " ['https://solo-leveling.fandom.com/wiki/Iron' '0.08056234850632071']\n",
      " ['https://solo-leveling.fandom.com/wiki/Groups' '0.08056234850632071']\n",
      " ['https://solo-leveling.fandom.com/wiki/Solo_Leveling_Wiki:Official_Policy'\n",
      "  '0.08056234850632071']\n",
      " ['https://solo-leveling.fandom.com/wiki/Solo_Leveling_Wiki:Improve_Pages'\n",
      "  '0.08056234850632071']\n",
      " ['https://solo-leveling.fandom.com/wiki/Class_Advancement_Quest%27s_Dungeon'\n",
      "  '0.08056234850632071']\n",
      " ['https://solo-leveling.fandom.com/wiki/Races' '0.08056234850632071']\n",
      " ['https://solo-leveling.fandom.com/wiki/Double_Dungeon'\n",
      "  '0.08056234850632071']]\n"
     ]
    }
   ],
   "source": [
    "def get_pages(num, page_ranks, pages):\n",
    "    # Procedure for obtaining the top 10 pages\n",
    "    n = page_ranks.shape[0]\n",
    "    pages = pages.reshape((n, 1))\n",
    "    ranks = np.hstack((pages, page_ranks))\n",
    "    ranks = ranks[ranks[:, 1].argsort()]\n",
    "    ranks = ranks[::-1]\n",
    "    return ranks[:num]\n",
    "\n",
    "page_ranks, eigen_value = get_eigenvalue_omega(M, 1)\n",
    "print(f'Eigenvalue: {eigen_value}')\n",
    "num = 10\n",
    "best = get_pages(num, page_ranks, web_sites)\n",
    "print(f'First {num} pages base on Google Page rank system and their scores:\\n {best}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
